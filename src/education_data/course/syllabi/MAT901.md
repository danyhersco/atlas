# Lecture 1: Introduction to Probability and Statistics

## What is Probability?

**Probability** is the branch of mathematics that deals with quantifying uncertainty. It allows us to model and analyze situations where the outcome is not deterministic, and instead, several outcomes are possible — but with different likelihoods.

### Example: Rolling a Die

Consider rolling a fair six-sided die. The possible outcomes are 1, 2, 3, 4, 5, and 6. Each outcome is equally likely. The probability of any specific outcome (say, rolling a 4) is:

$$
P(\text{rolling a 4}) = \frac{\text{number of favorable outcomes}}{\text{total outcomes}} = \frac{1}{6}
$$

### Everyday Examples

- **Weather Forecasting**: "There’s a 30% chance of rain tomorrow."
- **Medical Diagnosis**: Probabilities can state how likely a test is to detect a disease if it is present.
- **Games and Gambling**: How likely are you to win at a particular card game?

Probability provides the framework to model and reason about these uncertainties.

## What is Statistics?

**Statistics** is the science of collecting, analyzing, interpreting, and presenting data. While probability predicts the likelihood of future events, statistics involves analyzing data from the real world to learn about phenomena.

### Two Main Areas

- **Descriptive Statistics**: Methods to summarize or describe data (e.g., calculating the average height in a class).
- **Inferential Statistics**: Methods to make predictions or generalizations about a population based on a sample (e.g., polling 100 people to estimate national election results).

#### Example: Survey Results

Suppose you survey 100 students about their favorite fruit. If 40 say "banana", you use statistics to describe (descriptive) and, possibly, infer about all students (inferential).

## Applications in Real Life

Probability and statistics underpin modern decision-making in diverse fields:

- **Science and Engineering**: Analyzing experimental results, error estimation.
- **Business**: Market analysis, risk assessment.
- **Medicine**: Clinical trials, epidemiology.
- **Sports**: Player statistics, probability of winning.
- **Policy and Government**: Polls, census data, economic forecasts.
- **Artificial Intelligence**: Machine learning algorithms rely heavily on statistical inference and probability models.

**Example: Drug Effectiveness**

In clinical trials, probability helps determine the likelihood that observed results (e.g., recovery from a disease) are due to the new drug and not just random chance.

## Overview of the Course

In this course, we will cover:

- Foundations of probability
  - Basic probability rules and interpretations
  - Conditional probability and independence
  - Random variables and probability distributions
- Statistical methods
  - Summarizing data (mean, median, mode, variance)
  - Estimation and confidence intervals
  - Hypothesis testing
- Applications
  - Real-world scenarios involving randomness and data
  - Case studies in various fields

You will develop both intuition and technical skills to model uncertainty and make data-driven decisions.

### Course Structure & Assessment

- **Lectures**: Concept explanation and walkthroughs
- **Tutorials**: Problem-solving and discussions
- **Assignments/Quizzes**: Practice and assessment
- **Final Exam**: Comprehensive evaluation

A background in basic algebra is helpful, but no previous coursework in probability or statistics is required. All necessary fundamentals will be built up throughout the course.

## Probability vs. Statistics: Key Distinctions

| **Probability**                                            | **Statistics**                                                        |
|------------------------------------------------------------|----------------------------------------------------------------------|
| Starts with a known model, asks about likelihood of data   | Starts with data, seeks to infer about the processes or models        |
| Prospective: “Given a fair coin, what’s the probability of 7 heads in 10 tosses?” | Retrospective: “Given 7 heads in 10 tosses, is the coin fair?” |

## Example: Birthdays

### Probability Question

*What is the chance that, in a group of 23 people, at least two share a birthday?*

- This classic problem is solved by considering the possible arrangements and their probabilities.

### Statistics Question

*You survey a group and observe that two people share a birthday. Based on your data, is this unusual? What can you infer about birthdays’ distribution?*

## Basic Probability Notation

- $P(A)$: Probability of event A occurring
- $P(A \text{ and } B)$: Probability both A and B happen
- $P(A | B)$: Probability A occurs given that B has occurred

These notations will be used extensively.

## The Importance of Probability and Statistics

- Enables us to **reason quantitatively about uncertainty**.
- Drives evidence-based decisions in a world full of randomness.
- Essential in an age dominated by **big data** and **algorithms**.

### Famous Quote

> "Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write." — H.G. Wells

## Exercise 1.1

**Describe in your own words:**  
What is the difference between probability and statistics? Illustrate with an example from everyday life (other than the ones provided above).

## Exercise 1.2

Suppose you toss a fair coin 5 times.

- List all possible outcomes (as sequences of H for heads and T for tails).
- What is the probability of getting exactly 3 heads?

*Write out your process and reasoning.*

## Exercise 1.3

Recall a recent news headline or event where uncertainty or data analysis played a role (for example, a weather report, sports prediction, or election poll).

- How was probability or statistics (implicitly or explicitly) involved?
- Do you think the conclusions communicated were trustworthy? Why or why not?

## Exercise 1.4

Statistics is often used to summarize data. Find a small set of numbers, say the ages of your family members. 

- Compute the mean, median, and mode for your dataset.
- Which measure do you think best represents the "typical" age and why?

## Closing Example: A Simple Simulation (Python)

Suppose we simulate flipping a fair coin 100 times and count how many times it lands heads up. This illustrates how probability predicts outcomes, and how statistics helps us summarize results.

```python
import random

heads = 0
n_flips = 100
for _ in range(n_flips):
    if random.choice(['H', 'T']) == 'H':
        heads += 1
print("Number of heads:", heads)
print("Proportion of heads:", heads / n_flips)
```

- Run this code several times. How does the proportion of heads compare to what you’d *expect*? Why does it vary?

---

**For Next Time:**  
Next lecture, we’ll delve deeper into the rules of probability and how to calculate probabilities for combined and conditional events. Please attempt all exercises before then and bring your questions to class!

# Lecture 2: Basic Probability Concepts

## Sample Spaces and Events

In probability theory, everything starts with the notion of an *experiment*—any process or action that leads to an outcome. For example: tossing a coin, rolling a die, drawing a card from a deck.

- **Sample Space (S):** The set of all possible outcomes of an experiment.
  - Example: When tossing a fair coin, $S = \{\text{Heads}, \text{Tails}\}$
  - Example: When rolling a six-sided die, $S = \{1, 2, 3, 4, 5, 6\}$

- **Event:** Any subset of the sample space. Events may include one or more outcomes.
  - Example: Getting an even number when rolling a die, $E = \{2, 4, 6\}$

*Note:* The *empty set*, $\emptyset$, (no outcomes) and the entire sample space $S$ are also considered events.

**Exercise with Sample Spaces**

- Given the experiment: Drawing a card from a standard deck of 52 cards.
  - What is the sample space?
  - Give an example event: "Drawing a heart."

---

## Types of Events

Different relationships can exist between events:

### 1. **Simple Event**

Contains only one outcome.  
Example: Rolling a 3 on a die, $E = \{3\}$

### 2. **Compound Event**

Contains more than one outcome.
Example: Rolling an even number, $E = \{2, 4, 6\}$

### 3. **Mutually Exclusive Events**

Two events that cannot both happen at the same time.  
Example: On a die, $E_1 =$ rolling an odd number, $E_2 =$ rolling an even number. $E_1$ and $E_2$ are mutually exclusive.

### 4. **Exhaustive Events**

A set of events is exhaustive if one of them must occur. The collection covers the entire sample space.

### 5. **Independent Events**

Occurrence of one event does not affect the probability of the other.
Example: Toss two coins. Outcome of the first toss does not influence the outcome of the second.

### 6. **Dependent Events**

Occurrence of one event affects the probability of the other.

---

## Probability Rules

Probability assigns a numerical measure (from 0 to 1) to the likelihood of an event $E$ occurring. We denote this as $P(E)$.

### *Definition of Probability (for Equally Likely Outcomes)*

If all outcomes in a sample space are equally likely,

$$
P(E) = \frac{\text{Number of favorable outcomes to } E}{\text{Total number of possible outcomes}}
$$

#### Example:

If you roll a fair 6-sided die, what is the probability of rolling a number less than 5?

- Favorable outcomes: $\{1, 2, 3, 4\}$ (4 outcomes)
- Total outcomes: 6

So,

$$
P(\text{less than 5}) = \frac{4}{6} = \frac{2}{3}
$$

---

### The Addition Rule

#### For Mutually Exclusive Events:

If $A$ and $B$ are mutually exclusive (cannot happen together), then

$$
P(A \cup B) = P(A) + P(B)
$$

Example: Rolling a die, event $A =$ rolling a 2, event $B =$ rolling a 5.

- $P(A) = 1/6$
- $P(B) = 1/6$
- Since $A$ and $B$ can't happen together: $P(A \cup B) = 1/6 + 1/6 = 1/3$

#### For Non-Mutually Exclusive Events:

$$
P(A \cup B) = P(A) + P(B) - P(A \cap B)
$$

$A \cap B$ is the event "both A and B occur."

*Example*: Drawing a card from a deck. Let $A =$ event "drawing a heart," $B =$ event "drawing a king."

- Number of hearts: 13, so $P(A) = 13/52$
- Number of kings: 4, so $P(B) = 4/52$
- Number of kings of hearts: 1, so $P(A \cap B) = 1/52$

$$
P(A \cup B) = \frac{13}{52} + \frac{4}{52} - \frac{1}{52} = \frac{16}{52} = \frac{4}{13}
$$

---

### The Multiplication Rule

#### For Independent Events:

If $A$ and $B$ are independent, then

$$
P(A \cap B) = P(A) \times P(B)
$$

*Example:* Tossing two coins, what is the probability both show heads?

$$
P(\text{first coin heads}) = 1/2 \
P(\text{second coin heads}) = 1/2 \
P(\text{both heads}) = 1/2 \times 1/2 = 1/4
$$

#### For Dependent Events:

If occurrence of $A$ affects $B$, then

$$
P(A \cap B) = P(A) \times P(B|A)
$$

where $P(B|A)$ is the probability of $B$ given $A$ has occurred.

*Example:* Drawing two cards from a deck without replacement. What is the probability both are aces?

- Probability first card is an ace: $4/52$
- Probability second card is an ace, given first was: $3/51$
- Therefore: $P(\text{2 aces}) = 4/52 \times 3/51 = 1/221$

---

## Complementary Events

The *complement* of an event $A$ is the event "A does not occur," written as $A'$.

The sum of the probabilities of an event and its complement is 1:

$$
P(A) + P(A') = 1 \
P(A') = 1 - P(A)
$$

### Example:

Probability of rolling a number greater than 4 on a die:

- $A = \{5, 6\}$
- $P(A) = 2/6 = 1/3$
- $P(A') = 1 - 1/3 = 2/3$ (probability of rolling 4 or less)

---

## Exercise 2.1

**List all possible outcomes.**  
Suppose you toss a fair coin three times. Write out the sample space.

---

## Exercise 2.2

**Classify events.**  
For a single roll of a fair die:

1. Let $A$ = {rolling a 1 or 2}, $B$ = {rolling a 2, 3, or 4}. Identify:
   - Which events are mutually exclusive?
   - Which are not?

---

## Exercise 2.3

**Apply addition and multiplication rules.**

- What is the probability of drawing a queen or a spade from a standard deck of 52 cards?
- What is the probability of drawing two red cards in succession from the deck, *without* replacement?

---

## Exercise 2.4

**Working with complements.**

- If the chance of rain tomorrow is 0.3, what is the probability it does *not* rain?
- In a group of 10 people, what is the probability at least one person was born on a Monday? (*Hint: Find the probability none was born on a Monday and use the complement rule*)

---

## Exercise 2.5

**Challenge Problem.**

A box contains 3 red balls and 2 blue balls. You randomly select two balls one after the other *without replacement*. What is the probability you get exactly one red and one blue ball?

---

## Summary

In this lecture, you have learned:

- How to define and construct sample spaces and events.
- The classification of events (simple, compound, mutually exclusive, independent).
- How to apply the addition and multiplication rules, and to recognize when to use each.
- The importance of complementary events and how to use them in probability computations.

Take time to work through the exercises, and try to express problems you encounter in terms of well-defined sample spaces and events. This will be critical for all later work in probability and statistics!

# Lecture 3: Conditional Probability and Independence

## Conditional Probability Defined

In the last lecture, we discussed fundamental probability rules and the idea of events. Today, we will extend these concepts by introducing *conditional probability* and examine how the probability of an event may change when we know that another event has occurred.

### What is Conditional Probability?

**Conditional probability** is the probability of event $A$ occurring given that event $B$ has occurred. It is denoted by $P(A\mid B)$, read as "the probability of $A$ given $B$.”

#### Mathematical Definition

Suppose $A$ and $B$ are two events, and $P(B) > 0$. The conditional probability of $A$ given $B$ is:
$$
P(A\mid B) = \frac{P(A \cap B)}{P(B)}
$$
where $P(A \cap B)$ is the probability that both $A$ and $B$ happen.

#### Example

Suppose we have a standard deck of 52 cards. Let:
- $A$ = event that a randomly drawn card is a king,
- $B$ = event that the card is a face card (jack, queen, or king).

- $P(A) = \frac{4}{52} = \frac{1}{13}$
- $P(B) = \frac{12}{52} = \frac{3}{13}$
- $P(A \cap B) = P(\text{card is a king}) = \frac{4}{52}$ (since all kings are face cards)

So,
$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{4/52}{12/52} = \frac{4}{12} = \frac{1}{3}
$$
Thus, if you know you've drawn a face card, the probability it's a king is 1/3.

### Properties of Conditional Probability

- $P(A\mid B)$ is only defined if $P(B) > 0$.
- $P(A\mid B)$ may not equal $P(A)$; the knowledge that $B$ has occurred may affect the probability of $A$.


## Multiplication Rule for Dependent Events

Conditional probability gives us a way to extend the multiplication rule to any two events.

### The General Multiplication Rule

For any two events $A$ and $B$:
$$
P(A \cap B) = P(A\mid B)P(B)
$$
Alternatively:
$$
P(A \cap B) = P(B\mid A)P(A)
$$

#### Example

Suppose you roll two dice. Let:
- $A$: The first die shows a 3.
- $B$: The sum of the two dice is 5.

What is $P(A \cap B)$? 

First, $P(A) = 1/6$.

If the first die is 3, then the sum is 5 only if the second die is 2.

- $P(B \mid A) = P($Second die is 2$) = 1/6$
- $P(A \cap B) = P(B \mid A) \times P(A) = (1/6) \times (1/6) = 1/36$

## Independence of Events

### Definition

Two events $A$ and $B$ are called **independent** if knowing that one event occurs does **not** affect the probability of the other:

$$
P(A\mid B) = P(A)
$$

This also means:
$$
P(A \cap B) = P(A) \cdot P(B)
$$

### Examples

#### Example 1: Coin Toss

If you toss a fair coin twice:
- $A$: First toss is heads.
- $B$: Second toss is heads.

These are independent. Knowing the result of the first toss does *not* affect the probability of the second.

$P(A) = 1/2$, $P(B) = 1/2$, $P(A \cap B) = 1/4$.

Indeed, $P(A \cap B) = (1/2) \times (1/2) = 1/4$.

#### Example 2: Dependent Events

Suppose in a bag you have 2 red and 3 blue balls. You draw 2 balls one after another *without replacement*. Let:
- $A$: First ball is red.
- $B$: Second ball is red.

$A$ and $B$ are not independent:

- $P(A) = 2/5$
- If first ball is red (one less red in bag), remaining: 1 red and 3 blue. So $P(B\mid A) = 1/4$
- $P(A \cap B) = (2/5) \times (1/4) = 2/20 = 1/10$

But $P(B) = $ probability that the second ball is red without any knowledge of the first draw.

- Total possible pairs: $5 \times 4 = 20$.
- Ways both are red: $2 \times 1 = 2$.
- $P(B) = (2/20) + (3/20) = 5/20 = 1/4$ (as per the law of total probability).

So $P(B\mid A) = 1/4$ but $P(B) = 1/4$: Here, weirdly, they numerically match because of the drawn ball, but generally with more balls, $P(B\mid A)$ would not equal $P(B)$. Hence, most draws "without replacement" are **not independent**.

## Bayes' Theorem

Conditional probability allows us to update the likelihood of causes given observed evidence. This is the essence of **Bayes' Theorem.**

### Statement

Given events $A$ and $B$ with $P(B) > 0$,
$$
P(A\mid B) = \frac{P(B\mid A)P(A)}{P(B)}
$$

#### Where:
- $P(A)$: Prior probability of $A$
- $P(B\mid A)$: Likelihood of observing $B$ given $A$
- $P(B)$: Total probability of $B$
- $P(A\mid B)$: Posterior probability of $A$ given $B$

### Bayes' Theorem in Practice

Suppose a medical test is used to detect a rare disease. Let:
- $A$: Person has disease.
- $B$: Test is positive.

Suppose:
- 1% of people have the disease ($P(A) = 0.01$).
- Test correctly detects disease 99% of the time ($P(B\mid A) = 0.99$).
- Test gives a false positive 5% of the time ($P(B\mid A^c) = 0.05$, where $A^c$ is "no disease").

What is the probability a person actually has the disease if the test is positive ($P(A\mid B)$)?

Compute $P(B)$ using the law of total probability:

$$
P(B) = P(B\mid A)P(A) + P(B\mid A^c)P(A^c)\
= (0.99)(0.01) + (0.05)(0.99) = 0.0099 + 0.0495 = 0.0594
$$

Now,
$$
P(A\mid B) = \frac{0.99 \times 0.01}{0.0594} = \frac{0.0099}{0.0594} \approx 0.167
$$

Thus, even if you test positive, the probability you have the disease is only about 16.7%! This highlights the importance of considering the base rate of the disease.

## Exercise 3.1

A bag contains 6 green and 4 yellow balls. Two balls are drawn *without replacement*.

1. Find the probability that both balls are green.
2. What is the probability that the second ball is green given that the first was green?
3. Are the events "first ball is green" and "second ball is green" independent? Explain your reasoning.

---

## Exercise 3.2

Suppose you roll a fair six-sided die, and let $A$ be the event "an even number is rolled," and $B$ be the event "a number greater than 3 is rolled."

1. Compute $P(A)$, $P(B)$, and $P(A \cap B)$.
2. What is $P(A\mid B)$?
3. Are $A$ and $B$ independent? Justify your answer.

---

## Exercise 3.3

In a certain population, 5% of people are left-handed. A random person is selected.

1. Let $L$ be the event "the person is left-handed," and $R$ be "the person writes with their right hand." Are $L$ and $R$ independent? Why or why not?
2. Suppose further that 70% of left-handed people write with their left hand, and 1% of right-handed people also write with their left hand. What is the probability that a randomly selected person writes with their left hand?

---

## Exercise 3.4

A test for a disease is positive for 97% of those who have it and 2% of healthy people. The disease affects 0.5% of the population.

1. If a randomly selected person tests positive, what is the probability they actually have the disease? (Use Bayes' theorem.)
2. Suppose the test comes back positive for two independent tests. What is the probability that the person has the disease now? (Assume test outcomes are independent for those with/without the disease.)

---

## Exercise 3.5

Let $A$ and $B$ be events such that $P(A) = 0.4$, $P(B) = 0.5$, and $P(A \cap B) = 0.2$.

1. Find $P(A\mid B)$ and $P(B\mid A)$.
2. Are these events independent? Explain mathematically.

---

## Additional Example: Coding Bayes' Theorem

Let's put the computation into code! Suppose we have the test scenario from earlier:

- $P($disease$)=0.01$
- $P($positive$\mid$disease$)=0.99$
- $P($positive$\mid$no disease$)=0.05$

Here's how you'd compute $P($disease$\mid$positive$)$ in Python:

```python
p_disease = 0.01
p_no_disease = 1 - p_disease
p_pos_given_disease = 0.99
p_pos_given_no_disease = 0.05

p_pos = p_pos_given_disease * p_disease + p_pos_given_no_disease * p_no_disease
p_disease_given_pos = p_pos_given_disease * p_disease / p_pos

print(f"Probability of disease given positive test: {p_disease_given_pos:.3f}")
```

---

## Summary

- **Conditional probability** helps us compute probabilities under new information.
- The **multiplication rule** for probabilities generalizes as $P(A \cap B) = P(A\mid B)P(B)$.
- **Independence** means the occurrence of one event does not affect the probability of another.
- **Bayes' theorem** lets us "reverse" conditional probabilities, updating our belief in a cause given new evidence.

For next time, please review these exercises and come prepared to discuss real-world applications of conditional probability and independence!

# Lecture 4: Random Variables and Probability Distributions

## Introduction

In the previous lectures, we explored foundational probability concepts—sample spaces, types of events, core probability rules, conditional probability, and Bayes’ Theorem. In this lecture, we move deeper into the language of probability by introducing the concepts of **random variables** and their **probability distributions**. Understanding these ideas is essential for modeling uncertainty and analyzing real-world phenomena statistically.

---

## Random Variables

A **random variable** is a function that assigns a real number to each outcome in a sample space of a random experiment.

- **Notation**: We typically use uppercase letters like $X$, $Y$, $Z$ for random variables, and lowercase letters like $x$, $y$, $z$ for their possible values.

### Example

Suppose you toss a coin twice.

- Sample space: $S = \{HH, HT, TH, TT\}$
- Let $X$ = number of heads observed.

Then, $X$ is a random variable with possible values $0, 1, 2$ depending on the outcome.

---

## Discrete vs Continuous Random Variables

Random variables come in two types: **discrete** and **continuous**.

### Discrete Random Variables

- Take on a countable number of possible values.
- Common examples: number of heads in a series of coin tosses, the number rolled on a die.

**Example:**  
Let $X$ be the outcome when rolling a fair six-sided die.  
Possible values: $X \in \{1, 2, 3, 4, 5, 6\}$

### Continuous Random Variables

- Take on values in a continuous range or interval.
- Common examples: the exact height of a person, the time it takes for a reaction to complete.

**Example:**  
Let $Y$ be the time (in seconds) it takes for a certain chemical reaction to occur.  
Possible values: $Y \in [0, \infty)$

---

## Probability Mass Functions (PMFs)

A **Probability Mass Function (PMF)** describes the probability distribution of a discrete random variable.

- **Definition:** $p(x) = P(X = x)$ for all $x$ in the possible values of $X$.

#### Properties of a PMF

1. $p(x) \ge 0$ for all $x$
2. $\sum_x p(x) = 1$

### Example: Rolling a Fair Die

Let $X$ be the number rolled.

$$
p(x) = \begin{cases}
1/6 & \text{if } x = 1,2,3,4,5,6 \
0 & \text{otherwise}
\end{cases}
$$

### Visualizing a PMF

```python
import matplotlib.pyplot as plt

x_vals = [1, 2, 3, 4, 5, 6]
pmf = [1/6] * 6

plt.stem(x_vals, pmf, use_line_collection=True)
plt.xlabel('Value of X')
plt.ylabel('Probability')
plt.title('PMF of a Fair Six-Sided Die')
plt.show()
```

---

## Probability Density Functions (PDFs)

A **Probability Density Function (PDF)** describes the probability distribution of a continuous random variable.

- The PDF itself is not a probability; instead, the area under the curve over an interval gives the probability that the variable falls within that interval.

#### Properties of a PDF

1. $f(y) \ge 0$ for all $y$
2. $\int_{-\infty}^{\infty} f(y) dy = 1$

### Example: Uniform Distribution on $[0,1]$

$$
f(y) = \begin{cases}
1 & \text{if } 0 \le y \le 1 \
0 & \text{otherwise}
\end{cases}
$$

To find the probability that $Y$ falls between $0.2$ and $0.5$:

$$
P(0.2 \le Y \le 0.5) = \int_{0.2}^{0.5} 1 dy = 0.3
$$

### Plotting a PDF Example

```python
import numpy as np
import matplotlib.pyplot as plt

y = np.linspace(0, 1, 100)
f = np.ones_like(y)

plt.plot(y, f, label='PDF')
plt.fill_between(y, f, alpha=0.3)
plt.xlabel('Value of Y')
plt.ylabel('Density')
plt.title('Uniform(0,1) PDF')
plt.ylim(0, 1.2)
plt.show()
```

---

## Cumulative Distribution Functions (CDFs)

The **Cumulative Distribution Function (CDF)** gives the probability that a random variable is less than or equal to a certain value.

- **Discrete random variable:** $F(x) = P(X \le x) = \sum_{t \le x} p(t)$
- **Continuous random variable:** $F(y) = P(Y \le y) = \int_{-\infty}^{y} f(t) dt$

#### Properties of a CDF

1. $0 \le F(x) \le 1$
2. $F$ is non-decreasing
3. $\lim_{x \to -\infty} F(x) = 0$, $\lim_{x \to \infty} F(x) = 1$

### Example: Die Roll (Discrete)

$$
F(3) = P(X \le 3) = p(1) + p(2) + p(3) = \frac{1}{6} + \frac{1}{6} + \frac{1}{6} = 0.5
$$

### Example: Uniform(0,1) (Continuous)

$$
F(y) = 
\begin{cases}
0 & \text{if } y < 0 \
y & \text{if } 0 \le y \le 1 \
1 & \text{if } y > 1
\end{cases}
$$

---

## Deriving CDFs and PMFs/PDFs

- For discrete $X$, the PMF gives the CDF: $F(x) = \sum_{t \le x} p(t)$.
- For continuous $Y$, the PDF is the derivative of the CDF: $f(y) = \frac{d}{dy} F(y)$ (where $F$ is differentiable).

---

## Why Are Distributions Important?

Probability distributions “summarize” the likelihood of all possible outcomes of a random experiment.

**Applications:**
- Modeling games of chance
- Predicting weather events
- Reliability analysis
- Statistical inference (estimation, hypothesis testing, etc.)

---

## Common Distributions

- **Discrete:**
  - Bernoulli, Binomial, Poisson, Geometric
- **Continuous:**
  - Uniform, Normal (Gaussian), Exponential

Details of these will be covered in future lectures.

---

## Exercise 4.1

**Let $X$ be the number of heads in three tosses of a fair coin.**
1. List all possible values of $X$ and compute its PMF.
2. Calculate $P(X = 2)$.
3. Find the CDF values $F(0)$, $F(1)$, $F(2)$, and $F(3)$.

---

## Exercise 4.2

**Suppose $Y$ is a continuous random variable with PDF:**

$$
f(y) = \begin{cases}
2y & \text{if } 0 \le y \le 1 \
0 & \text{otherwise}
\end{cases}
$$

1. Verify that $f(y)$ is a valid PDF.
2. Find the CDF $F(y)$ for all real $y$.
3. Compute $P(0.25 \le Y \le 0.75)$.

---

## Exercise 4.3

**Short Answer:**

Explain in your own words:

a) The difference between a PMF and a PDF.

b) Why can't we have $P(Y = 0.5)$ for a continuous random variable $Y$?

---

## Exercise 4.4

**Let $Z$ be a discrete random variable with PMF:**

$$
p(z) = \begin{cases}
0.1 & z = 0 \
0.4 & z = 1 \
0.3 & z = 2 \
0.2 & z = 3 \
0 & \text{otherwise}
\end{cases}
$$

1. Check that $p(z)$ is a valid PMF.
2. Sketch, by hand or code, the graph of $p(z)$ and its CDF $F(z)$.

---

## Summary

- **A random variable** translates uncertain outcomes into numbers.
- **Discrete random variables** use PMFs; **continuous random variables** use PDFs.
- The **CDF** accumulates probabilities—a cornerstone for understanding random events.
- Mastering these concepts equips you for analyzing real data and interpreting uncertainty mathematically.

---

**Be sure to attempt the exercises before reviewing the solutions in the next class!**

# Lecture 5: Common Probability Distributions

## Introduction

In this lecture, we will explore some of the most frequently encountered probability distributions in statistics and applied sciences. By the end of this lecture, you will be able to:

- Recognize situations modeled by the Bernoulli, Binomial, Poisson, Normal, and Uniform distributions.
- Understand properties, formulas, and parameters associated with each.
- Compute probabilities and expected values for each distribution.
- Relate real-world scenarios to appropriate probability models.

---

## Bernoulli Distribution

The **Bernoulli distribution** is the simplest discrete probability distribution, modeling a single experiment (trial) that results in just one of two possible outcomes, commonly termed as "success" (1) and "failure" (0).

### Properties

- **Support**: $X \in \{0, 1\}$
- **Parameter**: $p$ = probability of success ($0 \leq p \leq 1$)

### Probability Mass Function (PMF)

$$
P(X = x) = p^x (1 - p)^{1 - x} \quad \text{for} \quad x \in \{0, 1\}
$$

### Mean and Variance

- **Mean** (Expected value): $E[X] = p$
- **Variance**: $Var(X) = p(1 - p)$

### Example

Suppose you toss a fair coin ($p = 0.5$) and define $X = 1$ if heads, $X = 0$ if tails.

- $P(X = 1) = 0.5$
- $P(X = 0) = 0.5$
- $E[X] = 0.5$
- $Var(X) = 0.25$

---

## Binomial Distribution

A **Binomial distribution** generalizes the Bernoulli by modeling the number of successes in a fixed number $n$ of independent Bernoulli trials, each with probability $p$ of success.

### Properties

- **Support**: $X \in \{0, 1, 2, ..., n\}$
- **Parameters**: $n$ = number of trials, $p$ = probability of success in each trial

### Probability Mass Function (PMF)

$$
P(X = k) = {n \choose k} p^k (1 - p)^{n - k}
$$

where ${n \choose k}$ is the binomial coefficient:
$$
{n \choose k} = \frac{n!}{k! (n - k)!}
$$

### Mean and Variance

- **Mean**: $E[X] = np$
- **Variance**: $Var(X) = np(1 - p)$

### Example

Tossing a fair coin 5 times, probability of getting exactly 3 heads:
- $n = 5, p = 0.5, k = 3$
- $P(X = 3) = {5 \choose 3} (0.5)^3 (0.5)^2 = 10 \cdot 0.125 \cdot 0.25 = 0.3125$

### Python Example

```python
from scipy.stats import binom

n = 5
p = 0.5
k = 3
prob = binom.pmf(k, n, p)
print(f"Probability of exactly 3 heads: {prob}")
```

---

## Poisson Distribution

The **Poisson distribution** models the number of times an event occurs in an interval of time or space, when these events occur with a known constant mean rate and independently of the time since the last event.

### Properties

- **Support**: $X = 0, 1, 2, ...$
- **Parameter**: $\lambda$ (lambda) = mean number of events per interval ($\lambda > 0$)

### Probability Mass Function (PMF)

$$
P(X = k) = \frac{e^{-\lambda} \lambda^k}{k!}, \text{ for } k = 0, 1, 2, ...
$$

### Mean and Variance

- **Mean**: $E[X] = \lambda$
- **Variance**: $Var(X) = \lambda$

### Example

If a website receives on average $\lambda = 2$ messages per minute, the probability of receiving exactly 3 messages in a minute:

$$
P(X = 3) = \frac{e^{-2} 2^3}{3!} \approx \frac{0.1353 \cdot 8}{6} \approx 0.180
$$

### Python Example

```python
from scipy.stats import poisson

lam = 2
k = 3
prob = poisson.pmf(k, lam)
print(f"Probability of exactly 3 messages: {prob}")
```

---

## Normal Distribution

The **Normal distribution** (often called the "Gaussian" distribution) is by far the most important continuous probability distribution. It models numerous natural and social phenomena.

### Properties

- **Support**: $X \in (-\infty, \infty)$
- **Parameters**: $\mu$ = mean, $\sigma$ = standard deviation ($\sigma > 0$)

### Probability Density Function (PDF)

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
$$

### Main Characteristics

- The graph is a symmetric bell-shaped curve centered at $\mu$.
- The **Standard Normal Distribution** is the case $\mu = 0, \sigma = 1$.

### Standardization ("Z-score")

To convert $X \sim N(\mu, \sigma^2)$ to a standard normal $Z$:
$$
Z = \frac{X - \mu}{\sigma}
$$

### Example

Heights of adult men in a city are normally distributed with mean $175$ cm and standard deviation $8$ cm.

Probability a randomly selected man is taller than 183 cm:

- $Z = (183 - 175)/8 = 1$
- Using normal tables or Python, $P(Z > 1) = 1 - P(Z \leq 1) \approx 0.1587$

### Python Example

```python
from scipy.stats import norm

mu = 175
sigma = 8
x = 183
prob = 1 - norm.cdf(x, mu, sigma)
print(f"Probability of height > 183 cm: {prob}")
```

---

## Uniform Distribution

**Uniform distributions** are the simplest continuous or discrete probability distributions, where every value in the support has equal probability.

### Discrete Uniform Distribution

- Rolling a fair die: $P(X = k) = 1/6$ for $k = 1,2,3,4,5,6$

### Continuous Uniform Distribution

If $X$ is uniformly distributed over $[a, b]$:

- **PDF**: $f(x) = \frac{1}{b - a}$ for $a \leq x \leq b$
- **Mean**: $E[X] = \frac{a + b}{2}$
- **Variance**: $Var(X) = \frac{(b - a)^2}{12}$

### Example

Random real number between 0 and 1: $f(x) = 1$ for $0 \leq x \leq 1$.

---

## Other Key Distributions (Overview)

- **Geometric Distribution**: Number of Bernoulli trials until the first success.
- **Exponential Distribution**: Time between events in a Poisson process.
- **Beta and Gamma Distributions**: Useful for modeling probabilities and waiting times.

These will be explored later in the course.

---

## Exercise 5.1

Suppose that a biased coin has probability $p = 0.3$ of showing heads. Let $X$ be the number of heads in $10$ independent flips of the coin.

**Tasks:**
1. Write down the PMF of $X$.
2. Calculate the probability that $X$ is exactly $4$.
3. What are the mean and variance of $X$?

---

## Exercise 5.2

A call center receives on average $5$ calls per hour. Assume the number of calls per hour follows a Poisson distribution.

**Tasks:**
1. What is the probability that exactly $3$ calls come in a given hour?
2. What is the probability that no calls come in an hour?
3. What is the expected number and variance of calls per hour?

---

## Exercise 5.3

Let $X$ be a continuous random variable uniformly distributed on the interval $[2, 8]$.

**Tasks:**
1. Write the PDF of $X$.
2. Calculate $P(4 < X < 6)$.
3. Compute $E[X]$ and $Var(X)$.

---

## Exercise 5.4

Heights of a population are normally distributed with mean $160$ cm and standard deviation $10$ cm.

**Tasks:**
1. What proportion of the population is taller than $175$ cm?
2. What is the probability that a randomly chosen person is between $150$ cm and $170$ cm tall?

---

## Exercise 5.5

**Application**: Think of a real or hypothetical situation from your field (or daily life) that can be modeled using one of the distributions covered today. State:
- The scenario
- The chosen distribution and why it fits
- The parameter values you would use

---

## Summary

Today, we explored several fundamental probability distributions:

- **Bernoulli and Binomial**: Model repeated experiments with two outcomes.
- **Poisson**: Model rare or random events in time or space.
- **Normal**: The go-to model for naturally occurring measures.
- **Uniform**: For situations with truly equal probability across the range.

Mastering when and how to use these distributions is crucial for probability and statistics. Refer to the exercises for practice and apply these concepts to real-world phenomena.

# Lecture 6: Descriptive Statistics and Data Analysis

---

## Introduction

In the preceding lectures, we examined foundational probability concepts and key probability distributions. We will now pivot to **descriptive statistics**: the toolkit for summarizing, organizing, and presenting data. Descriptive statistics help us understand large data sets by reducing them to a few key figures and useful graphics.

This lecture covers:

- Types of Data
- Measures of Central Tendency: Mean, Median, Mode
- Measures of Dispersion: Variance, Standard Deviation
- Visualizing Data with Histograms and Boxplots

---

## Types of Data

Understanding the **type of data** you’re working with is essential, because it guides which statistical analyses and visualizations are appropriate.

### 1. Qualitative (Categorical) Data

- **Nominal:** Categories with no logical order (e.g., colors: red, green, blue).
- **Ordinal:** Categories with an inherent order, but intervals between values are not meaningful (e.g., grades: A, B, C, D).

### 2. Quantitative (Numerical) Data

- **Discrete:** Countable values, often integers (e.g., number of students in a class).
- **Continuous:** Any value within a range, often measured (e.g., height, weight, time).

#### Examples

| Variable              | Type         | Subtype      |
|-----------------------|--------------|--------------|
| Temperature           | Quantitative | Continuous   |
| Days absent           | Quantitative | Discrete     |
| Type of fruit         | Qualitative  | Nominal      |
| Customer satisfaction | Qualitative  | Ordinal      |

---

## Measures of Central Tendency

Central tendency statistics summarize the "middle" or "center" of a data set.

### Mean

The **mean** is the average value:
$$
\text{Mean (} \bar{x} \text{)} = \frac{1}{n} \sum_{i=1}^n x_i
$$
Where $x_i$ are the data points and $n$ is the number of points.

**Example:**
Suppose the daily sales for a week are [20, 23, 21, 25, 22, 24, 22].  
Mean = (20 + 23 + 21 + 25 + 22 + 24 + 22) / 7 = 22.43

### Median

The **median** is the middle value when the data is ordered.

- If $n$ is odd, median is the middle value.
- If $n$ is even, median is the average of the two middle values.

**Example:**
For [20, 21, 22, 22, 23, 24, 25] (ordered), median = 22.

### Mode

The **mode** is the most frequently occurring value.

**Example:**  
For [20, 23, 21, 25, 22, 24, 22], the mode is 22 (which appears twice).

#### When to Use Which?
- **Mean:** Very sensitive to outliers.
- **Median:** Robust to extreme values.
- **Mode:** Useful for categorical data or detecting common values.

---

## Measures of Dispersion

Dispersion describes how spread out the data is. Two important measures are **variance** and **standard deviation**.

### Variance

Variance measures the average squared deviation from the mean:
$$
\text{Sample Variance (} s^2 \text{)} = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
$$

**Example:**
For [2, 4, 6]:
- Mean: $\bar{x} = 4$
- Variance: $((2-4)^2 + (4-4)^2 + (6-4)^2)/2 = (4 + 0 + 4)/2 = 4$

### Standard Deviation

The standard deviation is the square root of the variance:
$$
s = \sqrt{s^2}
$$

In the example above: $s = \sqrt{4} = 2$

### Range and Interquartile Range

- **Range:** Maximum value minus minimum value.
- **Interquartile Range (IQR):** Difference between the 75th and 25th percentile (Q3 - Q1).

#### Example Calculation

Data: [2, 4, 6, 8, 10]
- Range: 10 - 2 = 8
- For quartiles: Q1 = 4, Q3 = 8, so IQR = 8 - 4 = 4

---

## Visualizing Data

Statistics becomes much clearer with effective visualizations. Two common methods: **histograms** and **boxplots**.

### Histograms

A **histogram** shows the distribution of a quantitative variable by grouping data into bins and plotting the frequency of each bin.

**Example (Python):**
```python
import matplotlib.pyplot as plt

data = [2, 4, 4, 5, 6, 6, 6, 7, 8, 10]
plt.hist(data, bins=5, edgecolor='black')
plt.title('Histogram Example')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()
```

**Interpretation:**  
Histograms reveal the shape of the distribution (e.g., skewed, symmetric, multimodal).

### Boxplots

A **boxplot** (or box-and-whisker plot) summarizes:

- Minimum
- First Quartile (Q1, 25th percentile)
- Median
- Third Quartile (Q3, 75th percentile)
- Maximum

Boxplots are effective for visualizing spread and detecting outliers.

**Example (Python):**
```python
plt.boxplot(data, vert=False)
plt.title('Boxplot Example')
plt.xlabel('Value')
plt.show()
```

**Interpretation:**  
- The box shows the IQR.
- The line inside the box is the median.
- Whiskers extend to min and max (excluding outliers).

---

## Worked Example

Suppose a sample of exam scores is:

[65, 68, 70, 70, 71, 71, 72, 73, 75, 80]

**Step 1: Central Tendency**
- Mean: $(65 + 68 + 70 + 70 + 71 + 71 + 72 + 73 + 75 + 80)/10 = 71.5$
- Median: (5th and 6th values) $(71 + 71)/2 = 71$
- Mode: 70, 71 (appear twice each)

**Step 2: Dispersion**
- Range: $80 - 65 = 15$
- Variance and Standard Deviation:
    - Differences from mean: [-6.5, -3.5, -1.5, -1.5, -0.5, -0.5, 0.5, 1.5, 3.5, 8.5]
    - Squares: [42.25, 12.25, 2.25, 2.25, 0.25, 0.25, 0.25, 2.25, 12.25, 72.25]
    - Sum: 146.5
    - Variance: $146.5 / 9 = 16.28$
    - Std dev: $\sqrt{16.28} = 4.04$

**Step 3: Visualization**
- Histogram will show a peak around 70-73.
- Boxplot will reveal the spread and any outliers.

---

## Exercise 6.1

Given the dataset:  
[11, 14, 15, 15, 18, 20, 21, 22, 24, 30]

1. Determine:
    - Mean
    - Median
    - Mode

2. Calculate:
    - Range
    - Variance
    - Standard deviation

3. Identify if there are any outliers using the 1.5 * IQR rule.

---

## Exercise 6.2

Given the dataset:  
[1, 2, 2, 3, 4, 4, 4, 5, 5, 8]

a) Sketch a histogram by hand, labeling the frequency of each bin if you divide the data into bins: [1-2], [3-4], [5-6], [7-8].

b) Draw a boxplot by hand, showing the values of the minimum, Q1, median, Q3, and maximum.

---

## Exercise 6.3

Suppose you have a new dataset of employee salaries at a company:  
[30000, 32000, 34000, 35000, 37000, 39000, 120000]

a) Compute mean and median.  
b) Explain why the mean is not representative of the "typical" salary in this case.  
c) Which measure—mean or median—would you report as a summary of the typical salary? Why?

---

## Summary

- **Descriptive statistics** summarize and structure data.
- Type of data (categorical vs quantitative) determines analysis method.
- **Central tendency:** mean, median, mode.
- **Dispersion:** range, variance, standard deviation, IQR.
- **Visualization:** histograms (distribution), boxplots (spread/outliers).
- Choosing the right summary and visualization is critical for meaningful data analysis.

---

*Review the exercises and ensure you can compute these statistics and create basic visualizations before proceeding to inferential statistics in the next lecture.*

# Lecture 7: Sampling and Sampling Distributions

## Introduction

In statistical practice, we often wish to make inferences about a population (such as all residents of a country, all manufactured parts from a factory, or all students at a university). However, it's usually impractical or impossible to collect data on every member of the population. Instead, we collect data from a subset of the population, known as a *sample*, and use this information to draw conclusions about the whole.

Understanding how to collect these samples and what inferences we can reliably make from them is a cornerstone of statistics. This lecture introduces the WHY and HOW of sampling, types of sampling methods, and the important concept of *sampling distributions*, especially that of the mean. We finish with a derivation and illustration of the fundamental Central Limit Theorem (CLT).

---

## Why Sample?

- **Population**: The entire set of individuals or items we want to study.
- **Sample**: A subset of the population, selected for analysis.

### Reasons for Sampling
- **Cost and Convenience**: Collecting data from an entire population is often too expensive and time-consuming.
- **Destructive Testing**: Sometimes, testing units destroys them (e.g., crash-testing cars).
- **Timeliness**: Decisions often need to be made quickly; sampling provides faster results.
- **Data Accessibility**: Some populations are unobservable in their entirety (e.g., future customers).

When *properly* conducted, sampling allows us to estimate population parameters (e.g., the mean) with measurable reliability.

---

## Sampling Methods

How we select our sample affects whether our results can be generalized to the population. There are several key sampling methods:

### 1. Simple Random Sampling

Every member of the population has an **equal chance** of being selected. No bias in selection.

**Example:** Assign a unique number to each member of the population and use a random number generator to pick your sample.

```python
import random

population = list(range(1, 101))  # population of size 100
sample_size = 10
sample = random.sample(population, sample_size)
print("Random sample:", sample)
```

### 2. Systematic Sampling

Every $k$-th member of the population is selected after a random start.

**Procedure:**
- Randomly select a starting point from the first $k$ members
- Select every $k$-th member thereafter

**Example:** If there are 1000 people and you want a sample of 100, select every 10th person.

### 3. Stratified Sampling

The population is divided into **strata** (subgroups) sharing a characteristic (e.g., age, gender), and random samples are taken from each subgroup.

- Ensures representation from each important subgroup

**Example:** Suppose a population is 60% women and 40% men. You divide the population accordingly, then sample proportionally from each.

### 4. Cluster Sampling

Population is divided into clusters (usually naturally occurring), some clusters are randomly selected, and **all members** of chosen clusters are sampled.

**Example:** Schools in a city are clusters; select 5 schools at random and survey all students in those schools.

### 5. Convenience Sampling (Not Recommended)

Samples are taken from a group that is easy to reach. This method is prone to bias.

**Example:** Standing outside the library and surveying passers-by.

---

### Properties of a "Good" Sample

- **Representative**: Accurately reflects the population.
- **Random**: Every member had a chance of being selected.
- **Free from bias**: No systematic tendency to over-represent or under-represent parts of the population.

---

## What Is a Sampling Distribution?

Suppose we take many samples from the same population and for each sample calculate a statistic (like the mean).

- **Sampling distribution**: The probability distribution of a statistic (e.g., sample mean) calculated from *all possible samples* of a certain size from the population.

### Why Is This Important?

- The sampling distribution tells us how much the sample statistic is likely to **vary** from sample to sample.
- It provides the **basis for statistical inference** (such as confidence intervals and hypothesis testing).

---

## The Sampling Distribution of the Mean

Suppose:
- Population has mean $\mu$ and standard deviation $\sigma$
- We take samples of size $n$ and compute the mean $\bar{x}$ for each sample

The set of all possible $\bar{x}$ from all possible samples of size $n$ forms the *sampling distribution of the mean*.

### Key Properties

- **Mean of the sampling distribution**: $\mu_{\bar{x}} = \mu$ (same as the population mean)
- **Standard deviation of the sampling distribution (standard error)**:
  $$
  \sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}
  $$
  where $n$ is the sample size.

- As **sample size increases**, the standard error decreases: sample means are closer to the true mean.

---

## Central Limit Theorem (CLT)

**Statement:**  
No matter the shape of the original population distribution, as the sample size $n$ **increases**, the sampling distribution of the sample mean $\bar{x}$ approaches a *normal distribution*, with mean $\mu$ and standard deviation $\sigma / \sqrt{n}$.

### Why Is This Amazing?

- It lets us use *normal probability* to make inferences about means, **even when the underlying population is not normal!**
- Often, $n \geq 30$ is "large enough" for the CLT to apply in practice.

### Visual Example

Suppose we have a population distribution that is skewed (not normal):

- Draw many samples of size $n=2$: Distribution of sample means is *less* skewed.
- Draw many samples of size $n=30$: Distribution of sample means is *almost normal*.

```python
import numpy as np
import matplotlib.pyplot as plt

# Skewed population
population = np.random.exponential(scale=2, size=100000)

# Draw many random samples, each of size 30, and compute sample means
sample_means = [np.mean(np.random.choice(population, size=30, replace=False)) for _ in range(5000)]

plt.hist(sample_means, bins=50, color="skyblue", edgecolor="k", density=True)
plt.title("Sampling distribution of the mean (n=30) from a skewed population")
plt.xlabel("Sample mean")
plt.ylabel("Density")
plt.show()
```

> This histogram will look almost normal, despite the population being skewed.

---

## Practical Considerations

- **Larger sample sizes** give more precise estimates (smaller standard error).
- A properly drawn random sample is essential for valid inferences.
- The sample size needed for the CLT to "kick in" depends on the population's distribution (strongly skewed populations may require larger $n$).

---

## Exercise 7.1

**Short Answer:**  
Describe a real-world scenario where cluster sampling would be more practical than simple random sampling, and explain why.

---

## Exercise 7.2

**Conceptual Reasoning:**  
Imagine you have a population with a mean $\mu = 70$ and standard deviation $\sigma = 20$. You draw a sample of size $n = 25$.  
What is the expected mean and standard deviation of the sampling distribution of the mean?

---

## Exercise 7.3

**Calculation and Explanation:**  
Suppose the heights of students at a university are approximately normally distributed with mean 170 cm and standard deviation 8 cm.

a) What is the probability that the mean height of a sample of 36 students exceeds 172 cm?

b) Show all steps, including computation of standard error and normal probability.

---

## Exercise 7.4

**Programming Exercise:**  
Write a Python program that simulates drawing 1000 random samples (with replacement) of size 10 from a population consisting of numbers 1 to 100. For each sample, compute the sample mean. Plot the histogram of the sample means and comment on its shape.

---

## Exercise 7.5

**Discussion:**  
Under what circumstances can convenience sampling introduce serious bias? Give an example related to surveys on smartphone usage.

---

## Summary

- Sampling enables us to draw conclusions about populations without measuring every member.
- The method of sampling affects how well we can generalize results.
- The distribution of sample means, known as the sampling distribution, is crucial for inference.
- The Central Limit Theorem underpins much of statistical inference, justifying the use of normal-based techniques even when the population distribution is unknown or non-normal, so long as the sample is large.
- Understanding these concepts is foundational for the next steps: confidence intervals and hypothesis testing.

# Lecture 8: Estimation and Confidence Intervals

## Introduction

In previous lectures, we learned how to describe data and the theoretical underpinnings of probability and random variables. We also discussed how sample means behave, especially as the sample size increases, and were introduced to the Central Limit Theorem. 

Today, we turn our focus to statistical inference: **how we use sample data to draw conclusions about unknown population parameters**. We will cover *estimation*—including both point and interval estimates—and learn how to construct and interpret **confidence intervals**.

---

## Point Estimates

A **point estimate** is a single numerical value used to estimate an unknown population parameter.

**Example:**
- Suppose we want to estimate the average height of all students at a university.
    - The population mean (true value) is unknown.
    - We take a random sample of 100 students and calculate their mean height (e.g., 170.2 cm). This sample mean is a *point estimate* for the population mean.

**Common Point Estimators:**
- The sample mean $\bar{x}$ estimates the population mean $\mu$.
- The sample proportion $\hat{p}$ estimates the population proportion $p$.
- The sample variance $s^2$ estimates the population variance $\sigma^2$.

**Notation Recap:**
- Sample mean: $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$
- Sample proportion: $\hat{p} = \frac{x}{n}$, where $x$ = number of successes in $n$ trials

**Properties of Good Estimators:**
- **Unbiased:** The estimator's expected value equals the parameter.
- **Consistent:** As sample size increases, the estimator converges to the parameter.
- **Efficient:** The estimator has smaller variance among all unbiased estimators.

---

## Interval Estimates

A single value rarely tells the full story, especially since a sample is just one of many possibilities. **Interval estimates** provide a range of plausible values for the population parameter, reflecting uncertainty due to sampling variability.

A **confidence interval (CI)** is the most common type of interval estimate in statistics.

**General structure:**
$$
\text{Estimate} \pm (\text{Margin of Error})
$$

**Interpretation:**
- "We are X% confident that the true parameter lies within this interval."

---

## Constructing Confidence Intervals

The construction of a confidence interval depends on:
- The parameter being estimated (mean, proportion, etc.)
- The sample size ($n$)
- Whether the population standard deviation is known
- The desired level of confidence (e.g., 90%, 95%, 99%)

**Let’s start with the most common case:**

### Confidence Interval for the Mean ($\mu$), Population Standard Deviation Known

If the population standard deviation ($\sigma$) is known, and:
- The population is normal *or* $n$ is large ($n \geq 30$; by the Central Limit Theorem)

The confidence interval for the mean is:
$$
\bar{x} \pm z^* \frac{\sigma}{\sqrt{n}}
$$

- $\bar{x}$: sample mean
- $z^*$: critical value from the standard normal distribution for the desired confidence level
    - 90% CI: $z^* \approx 1.645$
    - 95% CI: $z^* \approx 1.96$
    - 99% CI: $z^* \approx 2.576$
- $\frac{\sigma}{\sqrt{n}}$: standard error of the mean

**Example:**
Suppose $\bar{x} = 80$, $\sigma = 10$, $n = 25$. Compute a 95% confidence interval for the mean.

Calculation:
- Standard error: $SE = \frac{10}{\sqrt{25}} = 2$
- Margin of error: $ME = 1.96 \times 2 = 3.92$
- Confidence interval: $80 \pm 3.92$ or $(76.08, 83.92)$

### Confidence Interval for the Mean ($\mu$), Population Standard Deviation Unknown

Usually, $\sigma$ is unknown and is estimated by the *sample standard deviation* $s$.

Then, we use the **$t$-distribution** with $n-1$ degrees of freedom:

$$
\bar{x} \pm t^* \frac{s}{\sqrt{n}}
$$

- $t^*$: critical value from the $t$-distribution for the desired confidence level and degrees of freedom $n-1$

**How to find $t^*$:** Use tables or statistical software (e.g., for $n=10$, df=9, 95% CI gives $t^* \approx 2.262$).

**Example:**
Sample of $n=16$, $\bar{x}=50$, $s=8$. Find 95% CI for the mean.

- Degrees of freedom: $15$
- $t^* \approx 2.131$ (from table)
- $SE = \frac{8}{4} = 2$
- $ME = 2.131 \times 2 = 4.262$
- Confidence interval: $50 \pm 4.262$ or $(45.738, 54.262)$

### Confidence Interval for a Proportion ($p$)

For a sample proportion $\hat{p}$ based on $n$ observations:

$$
\hat{p} \pm z^* \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
$$

- This formula is reliable when $n\hat{p} \geq 10$ and $n(1-\hat{p}) \geq 10$

**Example:**
In a survey of 200 people, 56 favored a new policy. Compute a 95% CI for the true proportion.

- $\hat{p} = \frac{56}{200} = 0.28$
- Standard error: $SE = \sqrt{\frac{0.28 \times 0.72}{200}} \approx 0.0312$
- Margin of error: $1.96 \times 0.0312 \approx 0.0611$
- Confidence interval: $0.28 \pm 0.0611$ or $(0.2189, 0.3411)$

---

## Interpretation of Confidence Intervals

This is very important:

- A 95% confidence interval generated from a sample does **not** mean there is a 95% probability that the population parameter is in the interval. 
- Instead, **if we repeated the study many times**, approximately 95% of the calculated intervals would contain the true parameter.

**Common Errors:**
- Don’t say "There’s a 95% chance that μ is between X and Y." Once the interval is computed, μ is fixed; the interval either contains μ or it does not.

---

**Effect of Confidence Level and Sample Size:**
- Higher confidence $\rightarrow$ wider interval (more certainty demands a larger margin)
- Larger sample size $\rightarrow$ narrower interval (more precision)

**Visual Example (in Python):**

Suppose we want to visualize intervals for repeated samples:

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)
population_mean = 100
population_sd = 15
n = 25
num_intervals = 30
intervals = []

for _ in range(num_intervals):
    sample = np.random.normal(population_mean, population_sd, n)
    mean_sample = np.mean(sample)
    se = population_sd / np.sqrt(n)
    conf_int = (mean_sample - 1.96 * se, mean_sample + 1.96 * se)
    intervals.append(conf_int)

plt.hlines(range(num_intervals), [l for l, u in intervals], [u for l, u in intervals])
plt.vlines(population_mean, -1, num_intervals, color='red', linestyles='dashed')
plt.xlabel('Mean')
plt.ylabel('Sample')
plt.title('Confidence Intervals for Multiple Samples')
plt.show()
```

---

## Summary Table: Common Confidence Interval Formulas

| Parameter         | Standard Error                       | Critical Value | Confidence Interval                           |
|-------------------|-------------------------------------|----------------|-----------------------------------------------|
| Mean ($\sigma$ known)       | $\frac{\sigma}{\sqrt{n}}$               | $z^*$         | $\bar{x} \pm z^* \frac{\sigma}{\sqrt{n}}$     |
| Mean ($\sigma$ unknown)     | $\frac{s}{\sqrt{n}}$                    | $t^*$         | $\bar{x} \pm t^* \frac{s}{\sqrt{n}}$          |
| Proportion ($p$)            | $\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$   | $z^*$         | $\hat{p} \pm z^* \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$ |

---

## Exercise 8.1

Suppose you draw a random sample of $n=36$ battery lifetimes from a normal population with unknown standard deviation. You calculate the sample mean lifetime as $120$ hours and the standard deviation as $12$ hours.

**Task:**
1. Construct a 95% confidence interval for the population mean battery lifetime.
2. Explain (in words) what this confidence interval means.

---

## Exercise 8.2

A poll of $500$ voters finds that $240$ say they plan to vote for Candidate A. 

**Task:**
1. Find a 90% confidence interval for the true proportion of voters who plan to vote for Candidate A.
2. Interpret the meaning of the resulting confidence interval in the context of this poll.

---

## Exercise 8.3

**Exploration:**

Suppose you increase the sample size in Exercise 8.1 to $n=100$ (with the same sample mean and standard deviation). 

- How does the width of the confidence interval change?
- Compute the new 95% confidence interval.

---

## Exercise 8.4

**True or False? Explain your reasoning.**

"If we calculate a 95% confidence interval for the mean weight of apples and get the interval (120g, 135g), this means there is a 95% probability that the true mean lies between 120g and 135g."

---

## Conclusion

In this lecture, you learned:
- The difference between point and interval estimates.
- How to construct and interpret confidence intervals for means and proportions.
- How sample size and confidence level affect interval width.

Confidence intervals are a foundation for most statistical inference tasks. Next time, we’ll explore hypothesis testing, which builds directly upon the concepts learned today.

---

# Lecture 9: Hypothesis Testing

## Introduction to Hypothesis Testing

Hypothesis testing is a formal statistical approach used to decide whether the data provides enough evidence to reject a given hypothesis about a population parameter. It lies at the heart of inferential statistics, bridging the gap between sample data and broader conclusions about populations. Understanding how to set up, perform, and interpret hypothesis tests is essential for anyone working with data.

### Key Concepts

- **Null Hypothesis ($H_0$):** The default claim or status quo that there is no effect or no difference.
- **Alternative Hypothesis ($H_1$ or $H_A$):** The claim that contradicts or challenges the null hypothesis, suggesting an effect or a difference.
- **Test Statistic:** A value calculated from the sample data that is used to decide whether to reject $H_0$.
- **p-value:** The probability of observing a test statistic as extreme as, or more extreme than, the one observed, under the assumption that $H_0$ is true.
- **Significance Level ($\alpha$):** The threshold for deciding when to reject $H_0$, typically 0.05 (5%).
- **Type I and Type II Errors:** Two possible errors in hypothesis decision-making.

---

## Null and Alternative Hypotheses

A **hypothesis** is a statement about a population parameter (like the mean or proportion).

### Null Hypothesis ($H_0$)

- Represents the "no change," "no effect," or status quo.
- The statement you test directly.
- Example: $H_0$: The mean exam score is 70 ($\mu = 70$).

### Alternative Hypothesis ($H_1$ or $H_A$)

- Represents what you want to support or demonstrate.
- Indicates the presence of an effect or a difference.
- Example: $H_A$: The mean exam score is not 70 ($\mu \neq 70$).

#### Types of Alternative Hypotheses

- **Two-tailed:** $H_A: \mu \neq 70$ (mean is different from 70).
- **Left-tailed:** $H_A: \mu < 70$ (mean is less than 70).
- **Right-tailed:** $H_A: \mu > 70$ (mean is greater than 70).

**Note:** The form of $H_A$ determines the "direction" of your test.

---

## Types of Errors in Hypothesis Testing

When testing hypotheses, two kinds of mistakes can be made:

### Type I Error

- **Definition:** Rejecting $H_0$ when it is actually true.
- **Probability:** Denoted by $\alpha$ (the significance level).
- **Consequence:** Claiming an effect exists when it does not (a "false positive").

### Type II Error

- **Definition:** Failing to reject $H_0$ when $H_A$ is true.
- **Probability:** Denoted by $\beta$.
- **Consequence:** Failing to detect a real effect (a "false negative").

**Summary Table:**

|                | $H_0$ True | $H_0$ False (i.e., $H_A$ True) |
|----------------|:----------:|:------------------------------:|
| Reject $H_0$   | Type I     | Correct Decision               |
| Fail to Reject $H_0$ | Correct Decision | Type II                |

**Power of the Test:** The probability of correctly rejecting $H_0$ when $H_A$ is true ($1 - \beta$).

---

## The p-value and Statistical Significance

### What is a p-value?

- The p-value is the probability, under $H_0$, of obtaining a result at least as extreme as the one actually observed.
- A low p-value indicates that the observed data are unlikely under $H_0$.

### Making Decisions

- **If $p$-value $< \alpha$:** Reject $H_0$ (statistically significant result).
- **If $p$-value $\geq \alpha$:** Do not reject $H_0$.

#### Typical $\alpha$ values:

- 0.05 (5%)
- 0.01 (1%)
- 0.10 (10%)

**Example:** If you compute a p-value of 0.03 and $\alpha = 0.05$, you reject the null hypothesis because $0.03 < 0.05$.

---

## Steps in Hypothesis Testing

1. **State the null and alternative hypotheses.**
2. **Choose the significance level ($\alpha$).**
3. **Calculate the appropriate test statistic.**
4. **Determine the p-value (or compare the test statistic to a critical value).**
5. **Make a decision:** Reject or fail to reject $H_0$.
6. **Interpret the results in context.**

---

## One-sample and Two-sample Tests

### One-sample Test

Used when you are comparing a single sample to a known value (e.g., population mean).

- **Example:** Is the mean weight of apples from an orchard different from 150 grams?

#### One-sample Z-test for the Mean

If the population variance is known and the sample is large:

$$
Z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}}
$$

Where:
- $\bar{x}$: sample mean
- $\mu_0$: hypothesized mean (from $H_0$)
- $\sigma$: population standard deviation
- $n$: sample size

#### One-sample t-test

If population variance is unknown:

$$
t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}
$$

Where $s$ is the sample standard deviation.

---

### Two-sample Test

Used when you compare two independent samples.

- **Example:** Is the average exam score different between two classes?

#### Two-sample t-test (Equal Variances Assumed)

$$
t = \frac{\bar{x}_1 - \bar{x}_2}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
$$

Where:
- $\bar{x}_1$, $\bar{x}_2$: sample means
- $n_1$, $n_2$: sample sizes
- $s_p$: pooled standard deviation:

$$
s_p = \sqrt{ \frac{ (n_1-1)s_1^2 + (n_2-1)s_2^2 }{n_1 + n_2 - 2} }
$$

**If variances are unequal, use Welch's t-test (adjusts for inequality).**

---

## Hypothesis Testing in Practice: An Example (One-sample t-test)

**Scenario:** A juice company claims their bottles contain, on average, 500 ml of juice. A random sample of 10 bottles shows a mean of 490 ml and a sample standard deviation of 15 ml. Has the average changed?

1. **Hypotheses:**

   - $H_0$: $\mu = 500$
   - $H_A$: $\mu \neq 500$

2. **Significance Level:** $\alpha = 0.05$

3. **t-score:**
   $$
   t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}} = \frac{490 - 500}{15/\sqrt{10}} = \frac{-10}{4.74} \approx -2.11
   $$

4. **Degrees of freedom:** $n-1 = 9$

5. **p-value:** Look up the two-tailed p-value for $t = -2.11$ and 9 df (use a t-table or calculator). Suppose p-value $\approx 0.063$.

6. **Decision:** $p$-value $= 0.063 > 0.05$; fail to reject $H_0$.

7. **Conclusion:** No strong evidence that the mean volume is different from 500 ml.

---

## Hypothesis Testing for Proportions

Suppose we want to test a claim about the proportion $p$ in a population.

#### Example: One-sample Z-test for Proportions

Let $p_0$ be the hypothesized proportion, $\hat{p}$ the sample proportion, and $n$ the sample size.

$$
Z = \frac{\hat{p} - p_0}{\sqrt{ p_0 (1-p_0) / n } }
$$

Use this test when samples are sufficiently large (rule of thumb: $n p_0 > 5$ and $n(1-p_0) > 5$).

---

## Understanding the "Statistical Significance" Caveat

- **Statistical significance** means the result is unlikely under $H_0$, not that it is important or practical.
- **Practical significance** relates to whether the effect is large enough to matter in real life.

---

## Exercise 9.1

**State the null and alternative hypotheses for each scenario:**

a. A school wants to test if their average test scores this year are higher than last year's national average of 74.

b. A manufacturer claims fewer than 2% of their lightbulbs are defective.

c. A beverage company is accused of selling bottles with under 1 liter of content. You want to test if the mean content is less than 1 liter.

---

## Exercise 9.2

**Classify each scenario as a Type I or Type II error:**

a. A new medicine is declared effective, but in reality, it has no effect.

b. A smoke detector fails to sound during a real fire.

c. A scientist fails to find evidence that a new fertilizer improves crop yield, even though it is genuinely better.

---

## Exercise 9.3

The average lifespan of a particular brand of tire is claimed to be 40,000 km. A quality control manager takes a sample of 25 tires, finding a sample mean of 38,500 km and a sample standard deviation of 3,000 km.

a. State the null and alternative hypotheses to test if the mean lifespan differs from 40,000 km.

b. Compute the test statistic.

c. At a significance level of 0.05, and assuming a two-tailed test, what is your decision? (Use $t$-table or calculator as needed.)

---

## Exercise 9.4

A political poll reports that 54% of a sample of 600 voters support a policy. Test whether the support differs from 50% (the null hypothesis), at a 0.05 significance level.

a. State $H_0$ and $H_A$.

b. Compute the test statistic.

c. Interpret the result.

---

## Exercise 9.5

**Short answer:** 

Why do statisticians prefer reporting p-values rather than simply stating "significant/not significant"? Provide your perspective.

---

## Further Reading

- Look up examples of one-tailed and two-tailed hypothesis tests in practical settings.
- Read about the limitations and criticisms of relying solely on p-values.

---

# Lecture 10: Applications and Real-World Examples

---

## Case Studies in Probability and Statistics

To understand the power of probability and statistics, let's look at real-world scenarios where these tools provide insights, guide decisions, and sometimes even save lives.

### 1. **Healthcare: Clinical Trials**

In medicine, clinical trials use statistical methods to determine if a new drug is effective.

**Example:**
Suppose a pharmaceutical company tests a new drug for lowering blood pressure. Participants are randomly divided into two groups: one receives the drug, the other a placebo. After the trial:

- The mean decrease in blood pressure for the drug group: 10 mmHg
- The mean decrease for the placebo group: 2 mmHg
- The standard deviation in both: 5 mmHg
- Each group has 100 participants

A two-sample hypothesis test helps determine if the observed difference is statistically significant or likely due to chance.

### 2. **Business: A/B Testing**

E-commerce sites frequently use **A/B testing** to improve user experience and sales. Here, two versions of a web page ("A" and "B") are shown to different visitors. The probability of conversion (e.g., purchasing, clicking a button) is compared.

**Example:**
- Version A: 100 out of 1,000 visitors buy a product (conversion rate = 10%)
- Version B: 130 out of 1,000 visitors buy a product (conversion rate = 13%)

Statistical tests help determine if the increase for Version B is significant.

### 3. **Quality Control in Manufacturing**

Manufacturers use **control charts** to monitor process variation and maintain product quality.

**Example:**
A factory produces bolts, and an inspector measures the length of a sample of bolts each hour. If the measurements fall outside predetermined control limits, the process might be malfunctioning, requiring intervention.

### 4. **Social Sciences: Survey Sampling**

Political polling uses random sampling and confidence intervals to estimate the percentage of people who support a candidate.

**Example:**
You survey 1,000 voters using random sampling and find 540 support Candidate X, giving a sample proportion of 0.54. A confidence interval quantifies the uncertainty in this estimate for the whole population.

---

## Interpreting Results and Making Decisions

Just finding a p-value or confidence interval is not enough—interpretation is crucial.

### Interpreting p-values

- **p-value < 0.05** (commonly used threshold): The result is statistically significant. There is evidence **against** the null hypothesis.
- **p-value >= 0.05**: The result is **not** statistically significant. There is **not enough evidence** to reject the null hypothesis.

**Important Caution:** Statistical significance does not mean practical significance! A very large sample size can make tiny, insignificant effects statistically "significant."

### Confidence Intervals

A 95% confidence interval for a mean of (8.6 mmHg, 11.4 mmHg) means:

> "We are 95% confident that the **true mean** decrease in blood pressure (for the population) is between 8.6 and 11.4 mmHg."

This does **not** mean "there is a 95% probability that the mean is in this interval"—once the interval is computed, it either contains the mean or it does not.

---

## Common Mistakes and Misinterpretations

Understanding typical mistakes helps you avoid costly errors.

### 1. **Confusing Correlation and Causation**

Just because two variables move together does **not** mean one causes the other.

**Example:**
Ice cream sales and drowning incidents might both increase in summer—correlated, but ice cream does **not** cause drowning.

### 2. **Misunderstanding p-values**

- A p-value **does not** tell you the probability that the null hypothesis is true.
- A p-value of 0.04 means: "Assuming the null hypothesis is true, there's a 4% chance of observing a result at least as extreme as this."

### 3. **Ignoring the Sampling Method**

Non-random samples (e.g., surveys only reaching internet users) might not represent the population—introducing **sampling bias**.

### 4. **Cherry-picking or “p-hacking”**

Trying multiple analyses and only reporting the most favorable result can lead to false discoveries.

### 5. **Over-interpretation of Statistical Significance**

Statistically significant results may not be **practically** significant, especially with large samples.

---

## Course Summary

Let's review the journey so far:

- **Probability** focused on quantifying uncertainty, from basic rules to advanced concepts like Bayes' theorem.
- **Statistics** centered on collecting data, summarizing (descriptive stats), and drawing inferences (inferential stats).
- **Random Variables and Distributions** gave us the language to model all sorts of real phenomena.
- **Sampling and Confidence Intervals** showed how we generalize from a sample to a population with measured uncertainty.
- **Hypothesis Testing** taught us a systematic approach for making and evaluating claims using data.

Key skills you should now have:

- Reason with probabilities and make predictions.
- Analyze data with an understanding of measures of center and spread.
- Choose and use probability distributions in real-world settings.
- Construct and interpret confidence intervals.
- Conduct and interpret hypothesis tests, including p-values and errors.

Above all: Statistics is both a toolbox for science and a filter for understanding information. Clear thinking about probability and statistics helps you make better decisions—even outside academia.

---

## Further Resources

**Books:**
- "The Art of Statistics" by David Spiegelhalter
- "Naked Statistics" by Charles Wheelan
- "Introduction to the Practice of Statistics" by Moore, McCabe, and Craig

**Online:**
- Khan Academy: Statistics and Probability modules
- Coursera: Introductory Statistics by Stanford
- The Royal Statistical Society: www.rss.org.uk

**Datasets for Practice:**
- UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/index.php
- Kaggle Datasets: https://www.kaggle.com/datasets

---

## Exercise 10.1

**Case Study Analysis**

Read the following scenario and answer the questions below.

*A new teaching method is tested in schools to improve math scores. 200 students are randomly assigned to two groups: Group 1 (traditional) and Group 2 (new method). After six months:*

- *Group 1 Mean Score: 75, SD: 10, n=100*
- *Group 2 Mean Score: 78, SD: 12, n=100*

a) What kind of statistical test would you use to assess whether the new method led to a significant difference in scores?

b) Interpret what a 95% confidence interval for the difference in means that does **not** contain zero tells you in this context.

---

## Exercise 10.2

**Misinterpretation Identification**

Each of the following contains a potential mistake or misunderstanding. For each, identify and explain what's wrong.

a) "A p-value of 0.03 means there's a 97% chance the null hypothesis is false."

b) "The mean client satisfaction score rose after the new policy, so the policy must have caused more satisfaction."

c) "We surveyed our web visitors and 90% liked the new layout, so 90% of all customers like it."

---

## Exercise 10.3

**Interpreting Confidence Intervals**

Suppose a study finds the average time it takes to run a 100m dash is 14.2 seconds, with a 95% confidence interval of (14.0, 14.4) seconds. Explain *precisely* what this confidence interval means.

---

## Exercise 10.4

**Critical Thinking: Practical vs. Statistical Significance**

A diet pill study with 3,000 adults finds an average weight loss of 0.4 kg compared to placebo, with p-value = 0.01. The manufacturer claims, "Our product delivers significant weight loss!" Critique this claim based on statistical vs. practical significance.

---

## Exercise 10.5

**Find an Application**

Choose a real-world context of your interest (sports, politics, engineering, biology, etc.). Describe a situation where probability or statistics could help with decision-making or understanding data. Briefly outline *how* a statistical method covered in the course could apply.

---

This marks the end of *Introduction to Probability and Statistics*. Continue exploring and questioning data—your statistical thinking will serve you for life!